{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4ebc5c8",
   "metadata": {},
   "source": [
    "# üìä An√°lise Explorat√≥ria de Dados (EDA)\n",
    "\n",
    "Este notebook realiza uma an√°lise explorat√≥ria dos dados coletados do X/Twitter e Reddit para an√°lise de sentimento.\n",
    "\n",
    "## Objetivos:\n",
    "- üîç Inspe√ß√£o visual dos dados antes do modelo\n",
    "- üìà Detectar outliers e vi√©s lingu√≠stico\n",
    "- üìä An√°lise de distribui√ß√µes e padr√µes\n",
    "- üñºÔ∏è Gerar visualiza√ß√µes para o artigo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96564ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports e configura√ß√£o\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from wordcloud import WordCloud\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configurar estilo\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Criar diret√≥rio de figuras se n√£o existir\n",
    "import os\n",
    "os.makedirs('../figures', exist_ok=True)\n",
    "\n",
    "print(\"üì¶ Bibliotecas carregadas com sucesso!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1121f41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar dados coletados\n",
    "import glob\n",
    "from pathlib import Path\n",
    "\n",
    "# Buscar arquivos de dados\n",
    "data_files = glob.glob('../data/raw/*.json') + glob.glob('../data/raw/*.csv')\n",
    "\n",
    "if not data_files:\n",
    "    print(\"‚ùå Nenhum arquivo encontrado em data/raw/\")\n",
    "    print(\"üîß Execute primeiro o notebook 01_coleta.ipynb\")\n",
    "else:\n",
    "    print(f\"üìÅ Arquivos encontrados: {len(data_files)}\")\n",
    "    for file in data_files:\n",
    "        print(f\"   üìÑ {Path(file).name}\")\n",
    "\n",
    "# Carregar dados (assumindo formato padr√£o)\n",
    "dfs = []\n",
    "for file in data_files:\n",
    "    try:\n",
    "        if file.endswith('.json'):\n",
    "            df_temp = pd.read_json(file, lines=True)\n",
    "        else:\n",
    "            df_temp = pd.read_csv(file)\n",
    "        \n",
    "        # Adicionar coluna de origem\n",
    "        df_temp['file_source'] = Path(file).stem\n",
    "        dfs.append(df_temp)\n",
    "        print(f\"‚úÖ {Path(file).name}: {len(df_temp)} registros\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao carregar {file}: {e}\")\n",
    "\n",
    "if dfs:\n",
    "    # Combinar todos os DataFrames\n",
    "    df = pd.concat(dfs, ignore_index=True)\n",
    "    print(f\"\\nüìä Dataset combinado: {len(df)} registros\")\n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado carregado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3091a43b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspe√ß√£o inicial dos dados\n",
    "print(\"üîç INSPE√á√ÉO INICIAL DOS DADOS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Informa√ß√µes b√°sicas\n",
    "print(f\"üìè Shape: {df.shape}\")\n",
    "print(f\"üìä Colunas: {list(df.columns)}\")\n",
    "print(f\"üíæ Mem√≥ria: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "\n",
    "# Primeiras linhas\n",
    "print(\"\\nüëÄ Primeiras 3 linhas:\")\n",
    "display(df.head(3))\n",
    "\n",
    "# Informa√ß√µes sobre tipos e valores nulos\n",
    "print(\"\\nüìã Informa√ß√µes sobre as colunas:\")\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c46233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identificar colunas de texto principal\n",
    "text_columns = []\n",
    "possible_text_cols = ['text', 'content', 'body', 'title', 'tweet', 'post']\n",
    "\n",
    "for col in df.columns:\n",
    "    if any(keyword in col.lower() for keyword in possible_text_cols):\n",
    "        text_columns.append(col)\n",
    "\n",
    "if not text_columns:\n",
    "    # Tentar detectar automaticamente\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' and df[col].str.len().mean() > 20:\n",
    "            text_columns.append(col)\n",
    "\n",
    "print(f\"üìù Colunas de texto identificadas: {text_columns}\")\n",
    "\n",
    "# Usar a primeira coluna de texto como principal\n",
    "if text_columns:\n",
    "    main_text_col = text_columns[0]\n",
    "    print(f\"üéØ Coluna principal: {main_text_col}\")\n",
    "    \n",
    "    # Adicionar coluna de comprimento do texto\n",
    "    df['text_length'] = df[main_text_col].astype(str).str.len()\n",
    "    \n",
    "    print(f\"üìè Estat√≠sticas de comprimento:\")\n",
    "    print(df['text_length'].describe())\n",
    "else:\n",
    "    print(\"‚ùå Nenhuma coluna de texto encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc9cb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. HISTOGRAMA DE TAMANHO DOS TEXTOS\n",
    "if text_columns:\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Histograma geral\n",
    "    ax1.hist(df['text_length'], bins=50, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    ax1.set_xlabel('Comprimento do Texto (caracteres)')\n",
    "    ax1.set_ylabel('Frequ√™ncia')\n",
    "    ax1.set_title('Distribui√ß√£o do Comprimento dos Textos')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Box plot por plataforma (se existir coluna de plataforma)\n",
    "    platform_cols = ['platform', 'source', 'file_source']\n",
    "    platform_col = None\n",
    "    \n",
    "    for col in platform_cols:\n",
    "        if col in df.columns:\n",
    "            platform_col = col\n",
    "            break\n",
    "    \n",
    "    if platform_col:\n",
    "        df.boxplot(column='text_length', by=platform_col, ax=ax2)\n",
    "        ax2.set_title('Comprimento por Plataforma')\n",
    "        ax2.set_ylabel('Comprimento do Texto')\n",
    "    else:\n",
    "        # Box plot geral\n",
    "        ax2.boxplot(df['text_length'])\n",
    "        ax2.set_title('Box Plot - Comprimento dos Textos')\n",
    "        ax2.set_ylabel('Comprimento do Texto')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/text_length_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üíæ Figura salva: figures/text_length_distribution.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08f58ff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. WORDCLOUD DOS TEXTOS\n",
    "if text_columns:\n",
    "    # Combinar todos os textos\n",
    "    all_text = ' '.join(df[main_text_col].astype(str).tolist())\n",
    "    \n",
    "    # Criar WordCloud\n",
    "    wordcloud = WordCloud(\n",
    "        width=800, \n",
    "        height=400, \n",
    "        background_color='white',\n",
    "        max_words=100,\n",
    "        colormap='viridis',\n",
    "        relative_scaling=0.5\n",
    "    ).generate(all_text)\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis('off')\n",
    "    plt.title('Word Cloud - Termos Mais Frequentes', fontsize=16, pad=20)\n",
    "    \n",
    "    plt.savefig('../figures/wordcloud.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üíæ Figura salva: figures/wordcloud.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e24a2bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. AN√ÅLISE TEMPORAL\n",
    "# Tentar identificar coluna de data\n",
    "date_columns = []\n",
    "possible_date_cols = ['date', 'created_at', 'timestamp', 'time']\n",
    "\n",
    "for col in df.columns:\n",
    "    if any(keyword in col.lower() for keyword in possible_date_cols):\n",
    "        date_columns.append(col)\n",
    "\n",
    "print(f\"üìÖ Colunas de data identificadas: {date_columns}\")\n",
    "\n",
    "if date_columns:\n",
    "    date_col = date_columns[0]\n",
    "    \n",
    "    # Converter para datetime se necess√°rio\n",
    "    try:\n",
    "        df[date_col] = pd.to_datetime(df[date_col])\n",
    "        \n",
    "        # Criar coluna de data (sem hora)\n",
    "        df['date_only'] = df[date_col].dt.date\n",
    "        \n",
    "        # An√°lise temporal por dia\n",
    "        daily_counts = df.groupby('date_only').size().reset_index(name='count')\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.plot(daily_counts['date_only'], daily_counts['count'], marker='o', linewidth=2)\n",
    "        plt.xlabel('Data')\n",
    "        plt.ylabel('N√∫mero de Posts')\n",
    "        plt.title('Distribui√ß√£o Temporal dos Posts')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.savefig('../figures/temporal_distribution.png', dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"üíæ Figura salva: figures/temporal_distribution.png\")\n",
    "        \n",
    "        # Estat√≠sticas temporais\n",
    "        print(f\"üìä Per√≠odo: {df[date_col].min()} at√© {df[date_col].max()}\")\n",
    "        print(f\"üìÖ Dias √∫nicos: {df['date_only'].nunique()}\")\n",
    "        print(f\"üìà M√©dia posts/dia: {daily_counts['count'].mean():.1f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao processar datas: {e}\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Nenhuma coluna de data encontrada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595c8c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. DETEC√á√ÉO DE IDIOMA\n",
    "try:\n",
    "    from langdetect import detect\n",
    "    \n",
    "    print(\"üîÑ Detectando idiomas...\")\n",
    "    \n",
    "    # Fun√ß√£o para detectar idioma de forma segura\n",
    "    def safe_detect_lang(text):\n",
    "        try:\n",
    "            return detect(str(text))\n",
    "        except:\n",
    "            return 'unknown'\n",
    "    \n",
    "    # Detectar idioma em uma amostra (para ser mais r√°pido)\n",
    "    sample_size = min(1000, len(df))\n",
    "    df_sample = df.sample(n=sample_size, random_state=42)\n",
    "    df_sample['detected_lang'] = df_sample[main_text_col].apply(safe_detect_lang)\n",
    "    \n",
    "    # Contar idiomas\n",
    "    lang_counts = df_sample['detected_lang'].value_counts()\n",
    "    \n",
    "    # Gr√°fico de pizza\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Mostrar apenas os 6 idiomas mais comuns\n",
    "    top_langs = lang_counts.head(6)\n",
    "    others = lang_counts.iloc[6:].sum()\n",
    "    \n",
    "    if others > 0:\n",
    "        top_langs['outros'] = others\n",
    "    \n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(top_langs)))\n",
    "    wedges, texts, autotexts = plt.pie(top_langs.values, \n",
    "                                       labels=top_langs.index, \n",
    "                                       autopct='%1.1f%%',\n",
    "                                       colors=colors,\n",
    "                                       startangle=90)\n",
    "    \n",
    "    plt.title(f'Distribui√ß√£o de Idiomas (amostra de {sample_size} posts)', fontsize=14)\n",
    "    \n",
    "    plt.savefig('../figures/language_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    # Estat√≠sticas\n",
    "    pt_percentage = (lang_counts.get('pt', 0) / sample_size) * 100\n",
    "    print(f\"üáßüá∑ Portugu√™s: {pt_percentage:.1f}% dos textos\")\n",
    "    print(f\"üåç Total de idiomas detectados: {len(lang_counts)}\")\n",
    "    \n",
    "    print(\"üíæ Figura salva: figures/language_distribution.png\")\n",
    "    \n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è langdetect n√£o instalado. Execute: pip install langdetect\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro na detec√ß√£o de idiomas: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a1432b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. DISTRIBUI√á√ÉO POR PLATAFORMA\n",
    "if platform_col:\n",
    "    platform_counts = df[platform_col].value_counts()\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Gr√°fico de barras\n",
    "    platform_counts.plot(kind='bar', ax=ax1, color='lightcoral')\n",
    "    ax1.set_title('Posts por Plataforma')\n",
    "    ax1.set_ylabel('N√∫mero de Posts')\n",
    "    ax1.tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    # Gr√°fico de pizza\n",
    "    ax2.pie(platform_counts.values, labels=platform_counts.index, autopct='%1.1f%%')\n",
    "    ax2.set_title('Distribui√ß√£o por Plataforma')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('../figures/platform_distribution.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"üíæ Figura salva: figures/platform_distribution.png\")\n",
    "    \n",
    "    # Estat√≠sticas por plataforma\n",
    "    print(\"\\nüìä Estat√≠sticas por plataforma:\")\n",
    "    for platform in platform_counts.index:\n",
    "        platform_data = df[df[platform_col] == platform]\n",
    "        avg_length = platform_data['text_length'].mean()\n",
    "        print(f\"   üì± {platform}: {len(platform_data)} posts, m√©dia {avg_length:.0f} chars\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Coluna de plataforma n√£o identificada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5abcb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. AN√ÅLISE DE OUTLIERS\n",
    "if text_columns:\n",
    "    print(\"üîç AN√ÅLISE DE OUTLIERS\")\n",
    "    print(\"=\" * 30)\n",
    "    \n",
    "    # Outliers por comprimento\n",
    "    Q1 = df['text_length'].quantile(0.25)\n",
    "    Q3 = df['text_length'].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    outliers = df[(df['text_length'] < lower_bound) | (df['text_length'] > upper_bound)]\n",
    "    \n",
    "    print(f\"üìè Outliers por comprimento: {len(outliers)} ({len(outliers)/len(df)*100:.1f}%)\")\n",
    "    \n",
    "    # Textos muito curtos\n",
    "    very_short = df[df['text_length'] < 10]\n",
    "    print(f\"üìù Textos muito curtos (<10 chars): {len(very_short)}\")\n",
    "    \n",
    "    # Textos muito longos\n",
    "    very_long = df[df['text_length'] > 1000]\n",
    "    print(f\"üìñ Textos muito longos (>1000 chars): {len(very_long)}\")\n",
    "    \n",
    "    # Mostrar exemplos\n",
    "    if len(very_short) > 0:\n",
    "        print(\"\\nüîç Exemplos de textos muito curtos:\")\n",
    "        for i, text in enumerate(very_short[main_text_col].head(3)):\n",
    "            print(f\"   {i+1}. '{text}' ({len(str(text))} chars)\")\n",
    "    \n",
    "    if len(very_long) > 0:\n",
    "        print(\"\\nüîç Exemplos de textos muito longos:\")\n",
    "        for i, text in enumerate(very_long[main_text_col].head(2)):\n",
    "            print(f\"   {i+1}. '{str(text)[:100]}...' ({len(str(text))} chars)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4545dee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. RELAT√ìRIO FINAL E EXPORTA√á√ÉO\n",
    "print(\"üìã RELAT√ìRIO FINAL DA EDA\")\n",
    "print(\"=\" * 35)\n",
    "\n",
    "# Resumo geral\n",
    "total_posts = len(df)\n",
    "avg_length = df['text_length'].mean() if 'text_length' in df.columns else 0\n",
    "unique_dates = df['date_only'].nunique() if 'date_only' in df.columns else 'N/A'\n",
    "platforms = df[platform_col].nunique() if platform_col else 'N/A'\n",
    "\n",
    "summary_stats = {\n",
    "    'total_posts': total_posts,\n",
    "    'avg_text_length': avg_length,\n",
    "    'unique_dates': unique_dates,\n",
    "    'platforms': platforms,\n",
    "    'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024**2\n",
    "}\n",
    "\n",
    "print(f\"üìä Total de posts: {total_posts:,}\")\n",
    "print(f\"üìè Comprimento m√©dio: {avg_length:.0f} caracteres\")\n",
    "print(f\"üìÖ Dias √∫nicos: {unique_dates}\")\n",
    "print(f\"üì± Plataformas: {platforms}\")\n",
    "print(f\"üíæ Uso de mem√≥ria: {summary_stats['memory_usage_mb']:.2f} MB\")\n",
    "\n",
    "# Salvar estat√≠sticas resumidas\n",
    "import json\n",
    "os.makedirs('../results', exist_ok=True)\n",
    "\n",
    "with open('../results/eda_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary_stats, f, indent=2, default=str)\n",
    "\n",
    "print(\"\\nüíæ Estat√≠sticas salvas em: results/eda_summary.json\")\n",
    "\n",
    "# Lista de figuras geradas\n",
    "figures_generated = [\n",
    "    'text_length_distribution.png',\n",
    "    'wordcloud.png',\n",
    "    'temporal_distribution.png',\n",
    "    'language_distribution.png',\n",
    "    'platform_distribution.png'\n",
    "]\n",
    "\n",
    "print(\"\\nüé® Figuras geradas:\")\n",
    "for fig in figures_generated:\n",
    "    fig_path = f\"../figures/{fig}\"\n",
    "    if os.path.exists(fig_path):\n",
    "        print(f\"   ‚úÖ {fig}\")\n",
    "    else:\n",
    "        print(f\"   ‚ùå {fig} (n√£o encontrada)\")\n",
    "\n",
    "print(\"\\nüéâ EDA Completa! Dados prontos para rotulagem e modelagem.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
