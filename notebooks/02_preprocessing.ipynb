{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2bcd752",
   "metadata": {},
   "source": [
    "# üßπ Notebook 2: Pr√©-processamento de Dados\n",
    "\n",
    "Este notebook demonstra o processo de pr√©-processamento dos dados coletados do Reddit, incluindo limpeza de texto, lematiza√ß√£o e prepara√ß√£o para an√°lise de sentimento.\n",
    "\n",
    "## üéØ Objetivos\n",
    "- Carregar dados brutos coletados\n",
    "- Aplicar limpeza e normaliza√ß√£o de texto\n",
    "- Realizar lematiza√ß√£o usando SpaCy\n",
    "- Filtrar dados por qualidade e relev√¢ncia\n",
    "- Salvar dados processados para pr√≥ximas etapas\n",
    "- Gerar estat√≠sticas de processamento\n",
    "\n",
    "## üìã Pr√©-requisitos\n",
    "- Dados coletados na etapa anterior\n",
    "- Modelo SpaCy PT-BR instalado: `python -m spacy download pt_core_news_sm`\n",
    "- Bibliotecas de processamento de texto instaladas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9c632c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports e configura√ß√£o inicial\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Adicionar src ao path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Configurar visualiza√ß√µes\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Verificar estrutura do projeto\n",
    "project_root = Path(\"..\").resolve()\n",
    "print(f\"üìÅ Diret√≥rio do projeto: {project_root}\")\n",
    "print(f\"üìÇ Estrutura principal:\")\n",
    "for item in [\"config\", \"src\", \"data\", \"scripts\"]:\n",
    "    path = project_root / item\n",
    "    status = \"‚úÖ\" if path.exists() else \"‚ùå\"\n",
    "    print(f\"  {status} {item}/\")\n",
    "\n",
    "print(\"\\nüîß Python path:\")\n",
    "for p in sys.path[-3:]:\n",
    "    print(f\"  - {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a166c8fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar configura√ß√£o\n",
    "config_path = project_root / \"config\" / \"topic.yaml\"\n",
    "\n",
    "try:\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"üìã Configura√ß√£o carregada:\")\n",
    "    print(f\"  üéØ T√≥pico: {config['topic']}\")\n",
    "    print(f\"  üîç Keywords: {config['keywords'][:5]}...\")  # Mostrar apenas primeiras 5\n",
    "    print(f\"  üîß Filtros:\")\n",
    "    print(f\"    - Comprimento: {config['filters']['min_length']}-{config['filters']['max_length']}\")\n",
    "    print(f\"    - Idioma: {config['filters']['language']}\")\n",
    "    print(f\"    - Score m√≠nimo: {config['filters']['min_score']}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"‚ùå Arquivo de configura√ß√£o n√£o encontrado: {config_path}\")\n",
    "    print(\"üí° Execute: cp config/topic.yaml.example config/topic.yaml\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Erro ao carregar configura√ß√£o: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02ec7d51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar importa√ß√£o das bibliotecas de preprocessamento\n",
    "print(\"üîç Testando importa√ß√£o das bibliotecas de pr√©-processamento:\")\n",
    "\n",
    "try:\n",
    "    from preprocessing.cleaner import TextCleaner\n",
    "    print(\"‚úÖ TextCleaner importado com sucesso\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erro ao importar TextCleaner: {e}\")\n",
    "\n",
    "try:\n",
    "    import spacy\n",
    "    # Testar se o modelo PT-BR est√° dispon√≠vel\n",
    "    nlp = spacy.load(\"pt_core_news_sm\")\n",
    "    print(\"‚úÖ SpaCy PT-BR dispon√≠vel\")\n",
    "except OSError:\n",
    "    print(\"‚ùå Modelo SpaCy PT-BR n√£o encontrado\")\n",
    "    print(\"üí° Instale com: python -m spacy download pt_core_news_sm\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erro ao importar SpaCy: {e}\")\n",
    "    print(\"üí° Instale com: pip install spacy\")\n",
    "\n",
    "try:\n",
    "    from langdetect import detect\n",
    "    print(\"‚úÖ langdetect dispon√≠vel\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ùå Erro ao importar langdetect: {e}\")\n",
    "    print(\"üí° Instale com: pip install langdetect\")\n",
    "\n",
    "# Verificar se DuckDB est√° dispon√≠vel para otimiza√ß√£o de parquet\n",
    "try:\n",
    "    import duckdb\n",
    "    print(\"‚úÖ DuckDB dispon√≠vel (otimiza√ß√£o de parquet)\")\n",
    "except ImportError:\n",
    "    print(\"‚ö†Ô∏è  DuckDB n√£o dispon√≠vel - usar√° pandas para parquet\")\n",
    "    print(\"üí° Para melhor performance: pip install duckdb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a6c9d6f",
   "metadata": {},
   "source": [
    "## üìä Explora√ß√£o dos Dados Brutos\n",
    "\n",
    "Vamos primeiro examinar os dados coletados para entender sua estrutura e qualidade."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67de9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descobrir arquivos de dados brutos\n",
    "raw_data_dir = project_root / \"data\" / \"raw\"\n",
    "\n",
    "print(f\"üìÅ Buscando dados em: {raw_data_dir}\")\n",
    "\n",
    "if not raw_data_dir.exists():\n",
    "    print(\"‚ùå Diret√≥rio de dados brutos n√£o encontrado\")\n",
    "    print(\"üí° Execute primeiro o notebook de coleta (01_coleta.ipynb)\")\n",
    "else:\n",
    "    # Buscar por arquivos CSV\n",
    "    csv_files = list(raw_data_dir.rglob(\"*.csv\"))\n",
    "    \n",
    "    if not csv_files:\n",
    "        print(\"‚ùå Nenhum arquivo CSV encontrado\")\n",
    "        print(\"üí° Execute primeiro a coleta de dados\")\n",
    "    else:\n",
    "        print(f\"üìÑ Encontrados {len(csv_files)} arquivo(s) CSV:\")\n",
    "        \n",
    "        for i, file in enumerate(csv_files, 1):\n",
    "            size_mb = file.stat().st_size / (1024*1024)\n",
    "            modified = datetime.fromtimestamp(file.stat().st_mtime)\n",
    "            print(f\"  {i}. {file.name}\")\n",
    "            print(f\"     üìè Tamanho: {size_mb:.2f} MB\")\n",
    "            print(f\"     üìÖ Modificado: {modified}\")\n",
    "            \n",
    "        # Selecionar arquivo mais recente por padr√£o\n",
    "        latest_file = max(csv_files, key=lambda x: x.stat().st_mtime)\n",
    "        print(f\"\\nüéØ Usando arquivo mais recente: {latest_file.name}\")\n",
    "        \n",
    "        # Carregar preview dos dados\n",
    "        try:\n",
    "            df_preview = pd.read_csv(latest_file, nrows=5)\n",
    "            print(f\"\\nüìä Preview dos dados ({latest_file.name}):\")\n",
    "            print(f\"Colunas: {list(df_preview.columns)}\")\n",
    "            \n",
    "            # Mostrar informa√ß√µes do dataset completo\n",
    "            df_full = pd.read_csv(latest_file)\n",
    "            print(f\"\\nüìà Estat√≠sticas do dataset:\")\n",
    "            print(f\"  üìä Total de registros: {len(df_full):,}\")\n",
    "            print(f\"  üìù Comprimento m√©dio do texto: {df_full['text'].str.len().mean():.1f} caracteres\")\n",
    "            print(f\"  üìÖ Per√≠odo dos dados: {df_full['timestamp'].min()} at√© {df_full['timestamp'].max()}\")\n",
    "            \n",
    "            if 'subreddit' in df_full.columns:\n",
    "                subreddit_counts = df_full['subreddit'].value_counts().head(5)\n",
    "                print(f\"  üîó Top 5 subreddits: {subreddit_counts.to_dict()}\")\n",
    "                \n",
    "            selected_file = latest_file\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå Erro ao carregar dados: {e}\")\n",
    "            selected_file = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e24a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An√°lise explorat√≥ria dos dados brutos\n",
    "if 'selected_file' in locals() and selected_file:\n",
    "    print(\"üîç Realizando an√°lise explorat√≥ria dos dados...\")\n",
    "    \n",
    "    # Carregar dados completos\n",
    "    df_raw = pd.read_csv(selected_file)\n",
    "    \n",
    "    print(f\"\\nüìä Informa√ß√µes gerais:\")\n",
    "    print(f\"  Registros: {len(df_raw):,}\")\n",
    "    print(f\"  Colunas: {len(df_raw.columns)}\")\n",
    "    print(f\"  Mem√≥ria usada: {df_raw.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
    "    \n",
    "    # An√°lise de qualidade dos dados\n",
    "    print(f\"\\nüîç Qualidade dos dados:\")\n",
    "    print(f\"  Valores nulos por coluna:\")\n",
    "    null_counts = df_raw.isnull().sum()\n",
    "    for col in null_counts.index:\n",
    "        if null_counts[col] > 0:\n",
    "            print(f\"    {col}: {null_counts[col]} ({null_counts[col]/len(df_raw)*100:.1f}%)\")\n",
    "    \n",
    "    # An√°lise de comprimento de texto\n",
    "    text_lengths = df_raw['text'].str.len()\n",
    "    print(f\"\\nüìè Distribui√ß√£o do comprimento do texto:\")\n",
    "    print(f\"  M√≠nimo: {text_lengths.min()} caracteres\")\n",
    "    print(f\"  M√©dio: {text_lengths.mean():.1f} caracteres\")\n",
    "    print(f\"  Mediano: {text_lengths.median():.1f} caracteres\")\n",
    "    print(f\"  M√°ximo: {text_lengths.max()} caracteres\")\n",
    "    \n",
    "    # Visualiza√ß√£o da distribui√ß√£o de comprimento\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(text_lengths, bins=50, alpha=0.7, color='skyblue')\n",
    "    plt.xlabel('Comprimento do texto (caracteres)')\n",
    "    plt.ylabel('Frequ√™ncia')\n",
    "    plt.title('Distribui√ß√£o do Comprimento do Texto')\n",
    "    plt.axvline(text_lengths.mean(), color='red', linestyle='--', label=f'M√©dia: {text_lengths.mean():.0f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    if 'score' in df_raw.columns:\n",
    "        plt.scatter(text_lengths, df_raw['score'], alpha=0.5, s=1)\n",
    "        plt.xlabel('Comprimento do texto')\n",
    "        plt.ylabel('Score do post')\n",
    "        plt.title('Comprimento vs Score')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Exemplos de texto\n",
    "    print(f\"\\nüìù Exemplos de textos:\")\n",
    "    sample_texts = df_raw['text'].sample(3).tolist()\n",
    "    for i, text in enumerate(sample_texts, 1):\n",
    "        print(f\"\\n  Exemplo {i} ({len(text)} chars):\")\n",
    "        print(f\"  {text[:200]}{'...' if len(text) > 200 else ''}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Nenhum arquivo de dados selecionado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbbb9a3",
   "metadata": {},
   "source": [
    "## üßπ Pr√©-processamento com TextCleaner\n",
    "\n",
    "Agora vamos aplicar o pr√©-processamento usando a classe TextCleaner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7943a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o TextCleaner e processar uma amostra\n",
    "if 'df_raw' in locals() and len(df_raw) > 0:\n",
    "    try:\n",
    "        # Inicializar TextCleaner\n",
    "        text_cleaner = TextCleaner(config)\n",
    "        print(\"‚úÖ TextCleaner inicializado com sucesso\")\n",
    "        \n",
    "        # Testar processamento com uma amostra pequena primeiro\n",
    "        print(\"\\nüß™ Testando processamento com amostra de 10 textos...\")\n",
    "        sample_df = df_raw.sample(n=min(10, len(df_raw))).copy()\n",
    "        \n",
    "        # Processar amostra\n",
    "        processed_sample = text_cleaner.process_dataframe(sample_df, 'text')\n",
    "        \n",
    "        print(f\"‚úÖ Processamento de amostra conclu√≠do\")\n",
    "        print(f\"  üìä Registros originais: {len(sample_df)}\")\n",
    "        print(f\"  üìä Registros v√°lidos: {len(processed_sample)}\")\n",
    "        \n",
    "        # Mostrar exemplo de processamento\n",
    "        if len(processed_sample) > 0:\n",
    "            print(\"\\nüìù Exemplo de processamento:\")\n",
    "            example = processed_sample.iloc[0]\n",
    "            print(f\"\\n  Original: {example['original'][:200]}...\")\n",
    "            print(f\"\\n  Limpo: {example['cleaned'][:200]}...\")\n",
    "            print(f\"\\n  Lematizado: {example['lemmatized'][:200]}...\")\n",
    "            print(f\"\\n  Comprimento: Original={len(example['original'])}, Limpo={len(example['cleaned'])}, Lematizado={len(example['lemmatized'])}\")\n",
    "        \n",
    "        # Estat√≠sticas do processamento\n",
    "        stats = text_cleaner.get_processing_stats(processed_sample)\n",
    "        print(f\"\\nüìà Estat√≠sticas do processamento:\")\n",
    "        for key, value in stats.items():\n",
    "            print(f\"  {key}: {value}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no processamento: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"‚ùå Dados brutos n√£o dispon√≠veis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21608c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processamento completo dos dados\n",
    "if 'text_cleaner' in locals() and 'df_raw' in locals():\n",
    "    print(f\"üöÄ Iniciando processamento completo de {len(df_raw):,} registros...\")\n",
    "    print(\"‚è±Ô∏è  Este processo pode levar alguns minutos...\")\n",
    "    \n",
    "    try:\n",
    "        # Processar todo o dataset\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        df_processed = text_cleaner.process_dataframe(df_raw, 'text')\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        processing_time = (end_time - start_time).total_seconds()\n",
    "        \n",
    "        print(f\"‚úÖ Processamento conclu√≠do em {processing_time:.1f} segundos\")\n",
    "        print(f\"  üìä Registros processados: {len(df_raw):,}\")\n",
    "        print(f\"  üìä Registros v√°lidos: {len(df_processed):,}\")\n",
    "        print(f\"  üìä Taxa de aprova√ß√£o: {len(df_processed)/len(df_raw)*100:.1f}%\")\n",
    "        print(f\"  ‚ö° Velocidade: {len(df_raw)/processing_time:.1f} textos/segundo\")\n",
    "        \n",
    "        # Estat√≠sticas detalhadas\n",
    "        final_stats = text_cleaner.get_processing_stats(df_processed)\n",
    "        print(f\"\\nüìà Estat√≠sticas finais:\")\n",
    "        for key, value in final_stats.items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"  {key}: {value:.1f}\")\n",
    "            else:\n",
    "                print(f\"  {key}: {value}\")\n",
    "        \n",
    "        # Compara√ß√£o antes/depois\n",
    "        print(f\"\\nüìä Compara√ß√£o antes/depois:\")\n",
    "        print(f\"  Comprimento m√©dio original: {df_raw['text'].str.len().mean():.1f} ‚Üí {df_processed['cleaned'].str.len().mean():.1f}\")\n",
    "        print(f\"  Registros: {len(df_raw):,} ‚Üí {len(df_processed):,} ({len(df_processed)/len(df_raw)*100:.1f}%)\")\n",
    "        \n",
    "        # Visualiza√ß√£o da distribui√ß√£o de comprimentos processados\n",
    "        plt.figure(figsize=(15, 5))\n",
    "        \n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.hist(df_raw['text'].str.len(), bins=50, alpha=0.7, label='Original', color='lightcoral')\n",
    "        plt.xlabel('Comprimento (caracteres)')\n",
    "        plt.ylabel('Frequ√™ncia')\n",
    "        plt.title('Texto Original')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.hist(df_processed['cleaned'].str.len(), bins=50, alpha=0.7, label='Limpo', color='lightgreen')\n",
    "        plt.xlabel('Comprimento (caracteres)')\n",
    "        plt.ylabel('Frequ√™ncia')\n",
    "        plt.title('Texto Limpo')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.hist(df_processed['lemmatized'].str.len(), bins=50, alpha=0.7, label='Lematizado', color='lightblue')\n",
    "        plt.xlabel('Comprimento (caracteres)')\n",
    "        plt.ylabel('Frequ√™ncia')\n",
    "        plt.title('Texto Lematizado')\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro no processamento completo: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "else:\n",
    "    print(\"‚ùå TextCleaner ou dados brutos n√£o dispon√≠veis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50207dec",
   "metadata": {},
   "source": [
    "## üíæ Salvando Dados Processados\n",
    "\n",
    "Vamos salvar os dados processados em formato Parquet para uso posterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54df67a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salvar dados processados\n",
    "if 'df_processed' in locals() and len(df_processed) > 0:\n",
    "    # Criar diret√≥rio de dados processados\n",
    "    processed_dir = project_root / \"data\" / \"processed\"\n",
    "    processed_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Gerar nome do arquivo com timestamp\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    topic_slug = config['topic'].lower().replace(' ', '_').replace('-', '_')\n",
    "    \n",
    "    # Salvar em diferentes formatos\n",
    "    parquet_file = processed_dir / f\"processed_{topic_slug}_{timestamp}.parquet\"\n",
    "    csv_file = processed_dir / f\"processed_{topic_slug}_{timestamp}.csv\"\n",
    "    \n",
    "    try:\n",
    "        print(f\"üíæ Salvando dados processados...\")\n",
    "        \n",
    "        # Salvar em Parquet (formato eficiente)\n",
    "        df_processed.to_parquet(parquet_file, index=False, compression='snappy')\n",
    "        parquet_size = parquet_file.stat().st_size / (1024*1024)\n",
    "        print(f\"‚úÖ Parquet salvo: {parquet_file.name} ({parquet_size:.2f} MB)\")\n",
    "        \n",
    "        # Salvar em CSV (para compatibilidade)\n",
    "        df_processed.to_csv(csv_file, index=False)\n",
    "        csv_size = csv_file.stat().st_size / (1024*1024)\n",
    "        print(f\"‚úÖ CSV salvo: {csv_file.name} ({csv_size:.2f} MB)\")\n",
    "        \n",
    "        print(f\"\\nüìä Compress√£o: CSV {csv_size:.1f} MB ‚Üí Parquet {parquet_size:.1f} MB ({parquet_size/csv_size*100:.1f}%)\")\n",
    "        \n",
    "        # Salvar metadados\n",
    "        metadata = {\n",
    "            'processed_at': datetime.now().isoformat(),\n",
    "            'original_file': str(selected_file.name) if 'selected_file' in locals() else 'unknown',\n",
    "            'original_records': len(df_raw) if 'df_raw' in locals() else 0,\n",
    "            'processed_records': len(df_processed),\n",
    "            'processing_time_seconds': processing_time if 'processing_time' in locals() else 0,\n",
    "            'config': config,\n",
    "            'statistics': final_stats if 'final_stats' in locals() else {}\n",
    "        }\n",
    "        \n",
    "        metadata_file = processed_dir / f\"metadata_{topic_slug}_{timestamp}.yaml\"\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(metadata, f, default_flow_style=False, allow_unicode=True)\n",
    "        \n",
    "        print(f\"‚úÖ Metadados salvos: {metadata_file.name}\")\n",
    "        \n",
    "        print(f\"\\nüéØ Arquivos gerados:\")\n",
    "        print(f\"  üìÑ {parquet_file.name} - Dados processados (recomendado)\")\n",
    "        print(f\"  üìÑ {csv_file.name} - Dados processados (CSV)\")\n",
    "        print(f\"  üìÑ {metadata_file.name} - Metadados do processamento\")\n",
    "        \n",
    "        # Verificar integridade dos dados salvos\n",
    "        print(f\"\\nüîç Verificando integridade...\")\n",
    "        df_test = pd.read_parquet(parquet_file)\n",
    "        assert len(df_test) == len(df_processed), \"Erro na verifica√ß√£o de integridade\"\n",
    "        print(f\"‚úÖ Integridade verificada - {len(df_test):,} registros\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao salvar dados: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Nenhum dado processado dispon√≠vel para salvar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b984f296",
   "metadata": {},
   "source": [
    "## üöÄ Execu√ß√£o via Script\n",
    "\n",
    "Alternativamente, voc√™ pode executar todo o pr√©-processamento usando o script dedicado:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89922d62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar pr√©-processamento usando o script preprocess.py\n",
    "import subprocess\n",
    "\n",
    "if 'selected_file' in locals() and selected_file:\n",
    "    # Diret√≥rio do arquivo selecionado\n",
    "    input_dir = selected_file.parent\n",
    "    \n",
    "    # Comando para executar o script de preprocessamento\n",
    "    cmd = [\n",
    "        \"python\", \n",
    "        str(project_root / \"scripts\" / \"preprocess.py\"),\n",
    "        \"--config\", str(project_root / \"config\" / \"topic.yaml\"),\n",
    "        \"--input-dir\", str(input_dir),\n",
    "        \"--output-dir\", str(project_root / \"data\" / \"processed\"),\n",
    "        \"--format\", \"parquet\",\n",
    "        \"--hash-users\",\n",
    "        \"--combine\",\n",
    "        \"--verbose\"\n",
    "    ]\n",
    "    \n",
    "    print(f\"üîß Comando do script:\")\n",
    "    print(f\"   {' '.join(cmd)}\")\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"üöÄ EXECUTANDO PR√â-PROCESSAMENTO VIA SCRIPT...\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    try:\n",
    "        # Executar o comando\n",
    "        result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(project_root))\n",
    "        \n",
    "        print(\"üì§ SA√çDA:\")\n",
    "        print(result.stdout)\n",
    "        \n",
    "        if result.stderr:\n",
    "            print(\"‚ö†Ô∏è  ERROS/AVISOS:\")\n",
    "            print(result.stderr)\n",
    "        \n",
    "        if result.returncode == 0:\n",
    "            print(\"‚úÖ Pr√©-processamento conclu√≠do com sucesso!\")\n",
    "            \n",
    "            # Verificar arquivos gerados\n",
    "            processed_dir = project_root / \"data\" / \"processed\"\n",
    "            if processed_dir.exists():\n",
    "                files = list(processed_dir.glob(\"*.parquet\"))\n",
    "                print(f\"üìÑ Arquivos processados gerados: {len(files)}\")\n",
    "                for file in files:\n",
    "                    size_mb = file.stat().st_size / (1024*1024)\n",
    "                    print(f\"  - {file.name}: {size_mb:.2f} MB\")\n",
    "                    \n",
    "                    # Verificar conte√∫do\n",
    "                    try:\n",
    "                        df_check = pd.read_parquet(file)\n",
    "                        print(f\"    üìä {len(df_check):,} registros processados\")\n",
    "                        if 'lemmatized' in df_check.columns:\n",
    "                            avg_length = df_check['lemmatized'].str.len().mean()\n",
    "                            print(f\"    üìè Comprimento m√©dio (lematizado): {avg_length:.1f} caracteres\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"    ‚ö†Ô∏è  Erro ao verificar arquivo: {e}\")\n",
    "        else:\n",
    "            print(f\"‚ùå Pr√©-processamento falhou com c√≥digo: {result.returncode}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Erro ao executar script: {e}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Nenhum arquivo de dados selecionado para processar\")\n",
    "    print(\"üí° Execute primeiro as c√©lulas de descoberta de dados\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c548b9",
   "metadata": {},
   "source": [
    "## üìä Resumo do Pr√©-processamento\n",
    "\n",
    "Este notebook realizou as seguintes etapas de pr√©-processamento:\n",
    "\n",
    "### ‚úÖ **Etapas Executadas:**\n",
    "1. **Limpeza de texto**: Remo√ß√£o de URLs, mentions, hashtags e caracteres especiais\n",
    "2. **Normaliza√ß√£o**: Convers√£o para min√∫sculas e remo√ß√£o de espa√ßos extras\n",
    "3. **Lematiza√ß√£o**: Redu√ß√£o das palavras √†s suas formas b√°sicas usando SpaCy PT-BR\n",
    "4. **Filtragem**: Aplica√ß√£o de filtros de comprimento, idioma e qualidade\n",
    "5. **Valida√ß√£o**: Verifica√ß√£o de integridade e qualidade dos dados processados\n",
    "\n",
    "### üìà **Pr√≥ximos Passos:**\n",
    "- **An√°lise Explorat√≥ria**: Examine os dados processados em detalhes\n",
    "- **Rotulagem**: Prepare dados para treinamento de modelos\n",
    "- **Modelagem**: Treine modelos de an√°lise de sentimento\n",
    "- **Avalia√ß√£o**: Teste e valide os modelos treinados\n",
    "\n",
    "### üìÅ **Arquivos Gerados:**\n",
    "- `processed_*.parquet`: Dados processados em formato eficiente\n",
    "- `processed_*.csv`: Dados processados em formato CSV\n",
    "- `metadata_*.yaml`: Metadados do processamento\n",
    "\n",
    "Os dados est√£o prontos para as pr√≥ximas etapas do pipeline de an√°lise de sentimento! üéâ"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
