{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8bc1bef5",
   "metadata": {},
   "source": [
    "# 📥 Notebook 1: Coleta de Dados - X/Twitter e Reddit\n",
    "\n",
    "Este notebook demonstra o processo de coleta de dados de redes sociais (X/Twitter e Reddit) para análise de sentimento em tópicos específicos.\n",
    "\n",
    "## 🎯 Objetivos\n",
    "- Configurar e testar coletores de dados\n",
    "- Coletar posts sobre um tópico específico\n",
    "- Validar qualidade dos dados coletados\n",
    "- Preparar dados para pré-processamento\n",
    "\n",
    "## 📋 Pré-requisitos\n",
    "- Credenciais do Reddit configuradas em `.env`\n",
    "- Ambiente Python com dependências instaladas\n",
    "- Configuração do tópico em `config/topic.yaml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c3b469",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports e configuração inicial\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import yaml\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "# Adicionar src ao path\n",
    "sys.path.append('../src')\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Verificar estrutura do projeto\n",
    "project_root = Path(\"..\").resolve()\n",
    "print(f\"📁 Diretório do projeto: {project_root}\")\n",
    "print(f\"📂 Estrutura principal:\")\n",
    "for item in [\"config\", \"src\", \"data\", \"scripts\"]:\n",
    "    path = project_root / item\n",
    "    status = \"✅\" if path.exists() else \"❌\"\n",
    "    print(f\"  {status} {item}/\")\n",
    "\n",
    "print(\"\\n🔧 Python path:\")\n",
    "for p in sys.path[-3:]:\n",
    "    print(f\"  - {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82051461",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carregar configuração do tópico\n",
    "config_path = project_root / \"config\" / \"topic.yaml\"\n",
    "\n",
    "try:\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    \n",
    "    print(\"📋 Configuração carregada:\")\n",
    "    print(f\"  🎯 Tópico: {config['topic']}\")\n",
    "    print(f\"  🔍 Keywords: {config['keywords']}\")\n",
    "    print(f\"  📊 Limites:\")\n",
    "    print(f\"    - Twitter: {config['limits']['twitter']}\")\n",
    "    print(f\"    - Reddit: {config['limits']['reddit']}\")\n",
    "    print(f\"  🔧 Filtros:\")\n",
    "    print(f\"    - Comprimento: {config['filters']['min_length']}-{config['filters']['max_length']}\")\n",
    "    print(f\"    - Idioma: {config['filters']['language']}\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(f\"❌ Arquivo de configuração não encontrado: {config_path}\")\n",
    "    print(\"💡 Execute: cp config/topic.yaml.example config/topic.yaml\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro ao carregar configuração: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d753432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testar importação dos scrapers\n",
    "try:\n",
    "    from scrapers.twitter import TwitterScraper\n",
    "    print(\"✅ TwitterScraper importado com sucesso\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Erro ao importar TwitterScraper: {e}\")\n",
    "\n",
    "try:\n",
    "    from scrapers.reddit import RedditScraper  \n",
    "    print(\"✅ RedditScraper importado com sucesso\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Erro ao importar RedditScraper: {e}\")\n",
    "\n",
    "try:\n",
    "    from utils.config import load_config, setup_logging\n",
    "    print(\"✅ Utilitários importados com sucesso\")\n",
    "except ImportError as e:\n",
    "    print(f\"❌ Erro ao importar utilitários: {e}\")\n",
    "\n",
    "# Verificar se todos os arquivos necessários existem\n",
    "required_files = [\n",
    "    \"src/scrapers/twitter.py\",\n",
    "    \"src/scrapers/reddit.py\", \n",
    "    \"src/utils/config.py\",\n",
    "    \".env.example\"\n",
    "]\n",
    "\n",
    "print(\"\\n📄 Verificando arquivos necessários:\")\n",
    "for file_path in required_files:\n",
    "    full_path = project_root / file_path\n",
    "    status = \"✅\" if full_path.exists() else \"❌\"\n",
    "    print(f\"  {status} {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3bcc9f",
   "metadata": {},
   "source": [
    "## 🐦 Teste de Coleta: X/Twitter\n",
    "\n",
    "Vamos testar a coleta de dados do X/Twitter usando `snscrape` com um limite pequeno para validação."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dcc1227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste de coleta X/Twitter com limite pequeno\n",
    "try:\n",
    "    from scrapers.twitter import TwitterScraper\n",
    "    \n",
    "    # Configurações de teste\n",
    "    test_keywords = config['keywords'][:2]  # Usar apenas 2 keywords para teste\n",
    "    test_limit = 10  # Limite pequeno para teste\n",
    "    \n",
    "    print(f\"🔍 Testando coleta com keywords: {test_keywords}\")\n",
    "    print(f\"📊 Limite: {test_limit} tweets\")\n",
    "    \n",
    "    # Inicializar scraper\n",
    "    twitter_scraper = TwitterScraper()\n",
    "    \n",
    "    # Realizar coleta de teste\n",
    "    tweets_data = twitter_scraper.collect(\n",
    "        keywords=test_keywords,\n",
    "        limit=test_limit,\n",
    "        lang='pt'\n",
    "    )\n",
    "    \n",
    "    if tweets_data:\n",
    "        print(f\"✅ Coleta bem-sucedida! {len(tweets_data)} tweets coletados\")\n",
    "        \n",
    "        # Converter para DataFrame para análise\n",
    "        df_tweets = pd.DataFrame(tweets_data)\n",
    "        print(f\"📊 Colunas: {list(df_tweets.columns)}\")\n",
    "        print(f\"📝 Exemplo de tweet:\")\n",
    "        if len(df_tweets) > 0:\n",
    "            print(f\"  Texto: {df_tweets.iloc[0]['text'][:100]}...\")\n",
    "            print(f\"  Data: {df_tweets.iloc[0]['timestamp']}\")\n",
    "            print(f\"  Usuário: {df_tweets.iloc[0].get('user_hash', 'N/A')}\")\n",
    "    else:\n",
    "        print(\"⚠️  Nenhum tweet coletado. Verifique as keywords ou conexão.\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro na coleta do Twitter: {e}\")\n",
    "    print(\"💡 Verifique se o snscrape está instalado: pip install snscrape\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "900384ae",
   "metadata": {},
   "source": [
    "## 🔴 Teste de Coleta: Reddit\n",
    "\n",
    "Vamos testar a coleta de dados do Reddit usando PRAW. **Importante**: Este teste requer credenciais configuradas no arquivo `.env`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2206c632",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificar credenciais do Reddit\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Carregar variáveis do .env\n",
    "env_path = project_root / \".env\"\n",
    "if env_path.exists():\n",
    "    load_dotenv(env_path)\n",
    "    print(\"✅ Arquivo .env encontrado\")\n",
    "else:\n",
    "    print(\"⚠️  Arquivo .env não encontrado\")\n",
    "    print(\"💡 Execute: cp .env.example .env e configure as credenciais\")\n",
    "\n",
    "# Verificar se as credenciais estão configuradas\n",
    "required_vars = [\"REDDIT_ID\", \"REDDIT_SECRET\", \"REDDIT_AGENT\", \"REDDIT_USERNAME\", \"REDDIT_PASSWORD\"]\n",
    "missing_vars = []\n",
    "\n",
    "print(\"\\n🔐 Verificando credenciais do Reddit:\")\n",
    "for var in required_vars:\n",
    "    value = os.getenv(var)\n",
    "    if value:\n",
    "        # Mostrar apenas primeiros caracteres para segurança\n",
    "        display_value = value[:4] + \"...\" if len(value) > 4 else value\n",
    "        print(f\"  ✅ {var}: {display_value}\")\n",
    "    else:\n",
    "        print(f\"  ❌ {var}: não configurado\")\n",
    "        missing_vars.append(var)\n",
    "\n",
    "if missing_vars:\n",
    "    print(f\"\\n⚠️  Variáveis faltando: {missing_vars}\")\n",
    "    print(\"💡 Configure no arquivo .env antes de continuar\")\n",
    "else:\n",
    "    print(\"\\n✅ Todas as credenciais estão configuradas!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33ebe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Teste de coleta Reddit com limite pequeno\n",
    "try:\n",
    "    from scrapers.reddit import RedditScraper\n",
    "    \n",
    "    # Verificar se as credenciais estão disponíveis\n",
    "    if not missing_vars:\n",
    "        print(\"🔍 Testando coleta do Reddit...\")\n",
    "        \n",
    "        # Configurações de teste\n",
    "        test_subreddits = config['reddit']['subreddits'][:1]  # Usar apenas 1 subreddit para teste\n",
    "        test_limit = 5  # Limite pequeno para teste\n",
    "        \n",
    "        print(f\"📊 Subreddits: {test_subreddits}\")\n",
    "        print(f\"📊 Limite: {test_limit} posts por subreddit\")\n",
    "        \n",
    "        # Inicializar scraper\n",
    "        reddit_scraper = RedditScraper()\n",
    "        \n",
    "        # Realizar coleta de teste\n",
    "        reddit_data = reddit_scraper.collect(\n",
    "            keywords=config['keywords'][:2],\n",
    "            subreddits=test_subreddits,\n",
    "            limit=test_limit\n",
    "        )\n",
    "        \n",
    "        if reddit_data:\n",
    "            print(f\"✅ Coleta bem-sucedida! {len(reddit_data)} posts coletados\")\n",
    "            \n",
    "            # Converter para DataFrame para análise\n",
    "            df_reddit = pd.DataFrame(reddit_data)\n",
    "            print(f\"📊 Colunas: {list(df_reddit.columns)}\")\n",
    "            print(f\"📝 Exemplo de post:\")\n",
    "            if len(df_reddit) > 0:\n",
    "                print(f\"  Título: {df_reddit.iloc[0]['title'][:100]}...\")\n",
    "                print(f\"  Data: {df_reddit.iloc[0]['timestamp']}\")\n",
    "                print(f\"  Subreddit: {df_reddit.iloc[0].get('subreddit', 'N/A')}\")\n",
    "                print(f\"  Score: {df_reddit.iloc[0].get('score', 'N/A')}\")\n",
    "        else:\n",
    "            print(\"⚠️  Nenhum post coletado. Verifique as keywords ou subreddits.\")\n",
    "            \n",
    "    else:\n",
    "        print(\"❌ Não é possível testar Reddit sem credenciais configuradas\")\n",
    "        print(\"💡 Configure o arquivo .env primeiro\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro na coleta do Reddit: {e}\")\n",
    "    print(\"💡 Verifique se o praw está instalado: pip install praw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08262d68",
   "metadata": {},
   "source": [
    "## 🚀 Coleta Completa\n",
    "\n",
    "Uma vez que os testes funcionaram, podemos executar a coleta completa usando os limites definidos na configuração."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202c0ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Executar coleta completa usando o script collector.py\n",
    "import subprocess\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Definir parâmetros da coleta\n",
    "twitter_limit = config['limits']['twitter']\n",
    "reddit_limit = config['limits']['reddit']\n",
    "topic = config['topic']\n",
    "\n",
    "print(f\"🎯 Iniciando coleta completa para: {topic}\")\n",
    "print(f\"📊 Limites: Twitter={twitter_limit}, Reddit={reddit_limit}\")\n",
    "\n",
    "# Criar timestamp para esta execução\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "output_dir = project_root / \"data\" / \"raw\" / timestamp\n",
    "\n",
    "print(f\"📁 Salvando em: {output_dir}\")\n",
    "\n",
    "# Comando para executar o collector\n",
    "cmd = [\n",
    "    \"python\", \n",
    "    str(project_root / \"scripts\" / \"collector.py\"),\n",
    "    \"--config\", str(project_root / \"config\" / \"topic.yaml\"),\n",
    "    \"--limit-twitter\", str(twitter_limit),\n",
    "    \"--limit-reddit\", str(reddit_limit),\n",
    "    \"--output-dir\", str(output_dir),\n",
    "    \"--verbose\"\n",
    "]\n",
    "\n",
    "print(f\"🔧 Comando: {' '.join(cmd)}\")\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🚀 EXECUTANDO COLETA...\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "try:\n",
    "    # Executar o comando\n",
    "    result = subprocess.run(cmd, capture_output=True, text=True, cwd=str(project_root))\n",
    "    \n",
    "    print(\"📤 SAÍDA:\")\n",
    "    print(result.stdout)\n",
    "    \n",
    "    if result.stderr:\n",
    "        print(\"⚠️  ERROS/AVISOS:\")\n",
    "        print(result.stderr)\n",
    "    \n",
    "    if result.returncode == 0:\n",
    "        print(\"✅ Coleta concluída com sucesso!\")\n",
    "        \n",
    "        # Verificar arquivos gerados\n",
    "        if output_dir.exists():\n",
    "            files = list(output_dir.glob(\"*.csv\"))\n",
    "            print(f\"📄 Arquivos gerados: {len(files)}\")\n",
    "            for file in files:\n",
    "                size_mb = file.stat().st_size / (1024*1024)\n",
    "                print(f\"  - {file.name}: {size_mb:.2f} MB\")\n",
    "        else:\n",
    "            print(\"⚠️  Diretório de saída não encontrado\")\n",
    "    else:\n",
    "        print(f\"❌ Coleta falhou com código: {result.returncode}\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Erro ao executar coleta: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
